{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client['article_recommendation']\n",
    "article_collection = db['article']\n",
    "\n",
    "# Find the first document in the collection\n",
    "first_article = article_collection.find_one()\n",
    "abstract = first_article['abstract']\n",
    "\n",
    "\n",
    "\n",
    "# Example text\n",
    "# text = \"This is an example sentence. John go to the school.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text\n",
    "doc = nlp(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization:\n",
    "Tokenization is the process of splitting text into individual words or tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AbstractIn\n",
      "this\n",
      "paper\n",
      ",\n",
      "we\n",
      "consider\n",
      "the\n",
      "problem\n",
      "of\n",
      "selection\n",
      "on\n",
      "coarse\n",
      "-\n",
      "grained\n",
      "distributed\n",
      "memory\n",
      "parallel\n",
      "computers\n",
      ".\n",
      "We\n",
      "discuss\n",
      "several\n",
      "deterministic\n",
      "and\n",
      "randomized\n",
      "algorithms\n",
      "for\n",
      "parallel\n",
      "selection\n",
      ".\n",
      "We\n",
      "also\n",
      "consider\n",
      "several\n",
      "algorithms\n",
      "for\n",
      "load\n",
      "balancing\n",
      "needed\n",
      "to\n",
      "keep\n",
      "a\n",
      "balanced\n",
      "distribution\n",
      "of\n",
      "data\n",
      "across\n",
      "processors\n",
      "during\n",
      "the\n",
      "execution\n",
      "of\n",
      "the\n",
      "selection\n",
      "algorithms\n",
      ".\n",
      "We\n",
      "have\n",
      "carried\n",
      "out\n",
      "detailed\n",
      "implementations\n",
      "of\n",
      "all\n",
      "the\n",
      "algorithms\n",
      "discussed\n",
      "on\n",
      "the\n",
      "CM-5\n",
      "and\n",
      "report\n",
      "on\n",
      "the\n",
      "experimental\n",
      "results\n",
      ".\n",
      "The\n",
      "results\n",
      "clearly\n",
      "demonstrate\n",
      "the\n",
      "role\n",
      "of\n",
      "randomization\n",
      "in\n",
      "reducing\n",
      "communication\n",
      "overhead\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Iterate over tokens\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech (POS) Tagging:\n",
    "POS tagging assigns a grammatical label to each token, such as noun, verb, adjective, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AbstractIn PROPN\n",
      "this DET\n",
      "paper NOUN\n",
      ", PUNCT\n",
      "we PRON\n",
      "consider VERB\n",
      "the DET\n",
      "problem NOUN\n",
      "of ADP\n",
      "selection NOUN\n",
      "on ADP\n",
      "coarse ADV\n",
      "- PUNCT\n",
      "grained VERB\n",
      "distributed VERB\n",
      "memory NOUN\n",
      "parallel ADJ\n",
      "computers NOUN\n",
      ". PUNCT\n",
      "We PRON\n",
      "discuss VERB\n",
      "several ADJ\n",
      "deterministic ADJ\n",
      "and CCONJ\n",
      "randomized ADJ\n",
      "algorithms NOUN\n",
      "for ADP\n",
      "parallel ADJ\n",
      "selection NOUN\n",
      ". PUNCT\n",
      "We PRON\n",
      "also ADV\n",
      "consider VERB\n",
      "several ADJ\n",
      "algorithms NOUN\n",
      "for ADP\n",
      "load NOUN\n",
      "balancing NOUN\n",
      "needed VERB\n",
      "to PART\n",
      "keep VERB\n",
      "a DET\n",
      "balanced ADJ\n",
      "distribution NOUN\n",
      "of ADP\n",
      "data NOUN\n",
      "across ADP\n",
      "processors NOUN\n",
      "during ADP\n",
      "the DET\n",
      "execution NOUN\n",
      "of ADP\n",
      "the DET\n",
      "selection NOUN\n",
      "algorithms VERB\n",
      ". PUNCT\n",
      "We PRON\n",
      "have AUX\n",
      "carried VERB\n",
      "out ADP\n",
      "detailed ADJ\n",
      "implementations NOUN\n",
      "of ADP\n",
      "all DET\n",
      "the DET\n",
      "algorithms NOUN\n",
      "discussed VERB\n",
      "on ADP\n",
      "the DET\n",
      "CM-5 PROPN\n",
      "and CCONJ\n",
      "report VERB\n",
      "on ADP\n",
      "the DET\n",
      "experimental ADJ\n",
      "results NOUN\n",
      ". PUNCT\n",
      "The DET\n",
      "results NOUN\n",
      "clearly ADV\n",
      "demonstrate VERB\n",
      "the DET\n",
      "role NOUN\n",
      "of ADP\n",
      "randomization NOUN\n",
      "in ADP\n",
      "reducing VERB\n",
      "communication NOUN\n",
      "overhead ADV\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "# Iterate over tokens with POS tags\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER):\n",
    "NER identifies named entities such as persons, organizations, locations, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AbstractIn PERSON\n",
      "CM-5 NORP\n"
     ]
    }
   ],
   "source": [
    "# Extract named entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-Removing Stopwords:\n",
    "Stopwords are common words (e.g., \"the\", \"is\", \"and\") that are often removed during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AbstractIn paper , consider problem selection coarse - grained distributed memory parallel computers . discuss deterministic randomized algorithms parallel selection . consider algorithms load balancing needed balanced distribution data processors execution selection algorithms . carried detailed implementations algorithms discussed CM-5 report experimental results . results clearly demonstrate role randomization reducing communication overhead .\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "filtered_tokens = [token.text for token in doc if token.text.lower() not in STOP_WORDS]\n",
    "\n",
    "# Join filtered tokens back into a sentence\n",
    "filtered_text = ' '.join(filtered_tokens)\n",
    "\n",
    "doc = nlp(filtered_text)\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out stopwords\n",
    "# filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "# filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AbstractIn paper consider problem selection coarse grained distributed memory parallel computers discuss deterministic randomized algorithms parallel selection consider algorithms load balancing needed balanced distribution data processors execution selection algorithms carried detailed implementations algorithms discussed CM-5 report experimental results results clearly demonstrate role randomization reducing communication overhead\n"
     ]
    }
   ],
   "source": [
    "# Filter out tokens that are not punctuation\n",
    "filtered_tokens = [token.text for token in doc if token.is_punct == False]\n",
    "\n",
    "# Join the filtered tokens into a string\n",
    "clean_text = \" \".join(filtered_tokens)\n",
    "doc = nlp(clean_text)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-Lemmatization:\n",
    "Lemmatization reduces words to their base or root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AbstractIn AbstractIn\n",
      "paper paper\n",
      "consider consider\n",
      "problem problem\n",
      "selection selection\n",
      "coarse coarse\n",
      "grained grain\n",
      "distributed distribute\n",
      "memory memory\n",
      "parallel parallel\n",
      "computers computer\n",
      "discuss discuss\n",
      "deterministic deterministic\n",
      "randomized randomized\n",
      "algorithms algorithm\n",
      "parallel parallel\n",
      "selection selection\n",
      "consider consider\n",
      "algorithms algorithm\n",
      "load load\n",
      "balancing balance\n",
      "needed need\n",
      "balanced balanced\n",
      "distribution distribution\n",
      "data datum\n",
      "processors processor\n",
      "execution execution\n",
      "selection selection\n",
      "algorithms algorithm\n",
      "carried carry\n",
      "detailed detailed\n",
      "implementations implementation\n",
      "algorithms algorithm\n",
      "discussed discuss\n",
      "CM-5 CM-5\n",
      "report report\n",
      "experimental experimental\n",
      "results result\n",
      "results result\n",
      "clearly clearly\n",
      "demonstrate demonstrate\n",
      "role role\n",
      "randomization randomization\n",
      "reducing reduce\n",
      "communication communication\n",
      "overhead overhead\n"
     ]
    }
   ],
   "source": [
    "# Iterate over tokens with lemmatized forms\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AbstractIn paper consider problem selection coarse grain distribute memory parallel computer discuss deterministic randomized algorithm parallel selection consider algorithm load balance need balanced distribution datum processor execution selection algorithm carry detailed implementation algorithm discuss CM-5 report experimental result result clearly demonstrate role randomization reduce communication overhead'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the sentence from lemmatized tokens\n",
    "lemmatized_abstract = \" \".join([token.lemma_ for token in doc])\n",
    "lemmatized_abstract"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "article_recommendation_system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
